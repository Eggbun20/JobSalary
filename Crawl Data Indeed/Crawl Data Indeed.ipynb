{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b48daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import time\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print('- Finish importing package')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af897ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Chrome\n",
    "\n",
    "DRIVER_PATH = '/Users/humi.mochi/Downloads/chromedriver'\n",
    "driver = webdriver.Chrome(executable_path = DRIVER_PATH)\n",
    "print('- Finish opening Chrome')\n",
    "sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d3300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import email and password\n",
    "# credential = open(\"login_topcv.txt\") #Create a file .txt contain private information Login\n",
    "# line = credential.readlines()\n",
    "# email = line[0]\n",
    "# password = line[1]\n",
    "# print('- Finish importing the login credentials')\n",
    "\n",
    "# # Key in email\n",
    "# email_field = driver.find_element(By.XPATH,'Copy xpath')\n",
    "# email_field.send_keys(email)\n",
    "# print('- Finish keying in email')\n",
    "# sleep(3)\n",
    "\n",
    "# #Key in password\n",
    "# password_field = driver.find_element(By.XPATH,'Copy xpath')\n",
    "# password_field.send_keys(password)\n",
    "# print('_ Finish keying in password')\n",
    "# sleep(3)\n",
    "\n",
    "# # Click login button\n",
    "# login_field = driver.find_element(By.XPATH,'Copy xpath')\n",
    "# login_field.click()\n",
    "# print('- Finish logging in')\n",
    "# sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a9dbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login Indeed login site\n",
    "\n",
    "url = 'https://www.indeed.com/stc?_ga=2.40830050.1924450139.1656265669-1082206623.1656265669'\n",
    "driver.get(url)\n",
    "print('- Finish getting url')\n",
    "sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7b1813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Record time run crawl data\n",
    "\n",
    "starttime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3b8ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and execute keyword searching\n",
    "\n",
    "def search_keyword(word):\n",
    "    '''\n",
    "        Func uses for input and execute keyword searching\n",
    "        Params: a specific word\n",
    "    '''\n",
    "    search_job = driver.find_element(By.XPATH, '//*[@id=\"text-input-what\"]')\n",
    "    search_job.send_keys(word)\n",
    "    search_button = driver.find_element(By.XPATH, '//*[@id=\"jobsearch\"]/button')\n",
    "    search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a356c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to the next page\n",
    "\n",
    "def goto_nextpage(word, page):\n",
    "    '''\n",
    "        Func uses to navigate to the next page\n",
    "        Params: a keyword, page number\n",
    "    '''\n",
    "    if ' ' in word:\n",
    "        uri = word.split(' ')\n",
    "        word1 = uri[0]\n",
    "        word2 = uri[1]\n",
    "        try:\n",
    "            next_page = driver.get(f'https://www.indeed.com/jobs?q={word1}%20{word2}&start={(page)*10}')\n",
    "\n",
    "        except:\n",
    "            next_page = driver.get(f'https://www.indeed.com/jobs?q={word1}%20{word2}&start={(page)*10}')\n",
    "    else:\n",
    "        try:\n",
    "            next_page = driver.get(f'https://www.indeed.com/jobs?q={word}&start={(page)*10}')\n",
    "\n",
    "        except:\n",
    "            next_page = driver.get(f'https://www.indeed.com/jobs?q={word}&start={(page)*10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66304393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize count variable to order csv files\n",
    "count = 1\n",
    "\n",
    "# Input a list of keywords\n",
    "# keywords = input_keywords()\n",
    "keywords = ['data science', 'development', 'data analyst', 'data engineer']\n",
    "for keyword in keywords:\n",
    "\n",
    "    titles = []\n",
    "    companies = []\n",
    "    locations =[]\n",
    "    salaries = []\n",
    "    types = []\n",
    "    categories = []\n",
    "\n",
    "    # Then print the new keyword to keep track\n",
    "    print(keyword)\n",
    "    \n",
    "    # Execute searching keyword and return the first result page\n",
    "    search_keyword(keyword)\n",
    "    \n",
    "    # First page\n",
    "    page = 1\n",
    "\n",
    "    # Lopp from page 1 to page 65\n",
    "    for page in range(1, 66):\n",
    "        # Print page number to console\n",
    "        print(f'{keyword} - Page {page}')\n",
    "        \n",
    "        # Find the list of job cards\n",
    "        job_card = driver.find_elements(By.XPATH, '//div[contains(@class,\"job_seen_beacon\")]')\n",
    "        \n",
    "        # Iterate to job cards\n",
    "        for job in job_card:\n",
    "            # Find DOM element that contains job's tilte\n",
    "            try:\n",
    "                title  = job.find_element(By.XPATH,'//*[@class=\"jobTitle\"]/a/span').text\n",
    "            except:\n",
    "                # If cannot find the element, set title is NULL\n",
    "                title  = None\n",
    "                # Append the title text to list\n",
    "            titles.append(title)\n",
    "\n",
    "            # The belows are same as finding title\n",
    "            try:    \n",
    "                company  = job.find_element(By.XPATH,'.//span[@class=\"companyName\"]').text\n",
    "            except:\n",
    "                company  = None\n",
    "            companies.append(company)\n",
    "\n",
    "            try:\n",
    "                location  = job.find_element(By.XPATH,'.//div[@class=\"companyLocation\"]').text\n",
    "            except:\n",
    "                location  = None\n",
    "            locations.append(location)\n",
    "\n",
    "            try:\n",
    "                salary = job.find_element(By.XPATH,'.//span[@class=\"estimated-salary\"]').text\n",
    "            except:\n",
    "                salary = None\n",
    "            salaries.append(salary)\n",
    "\n",
    "            try:\n",
    "                type = job.find_element(By.XPATH,'.//div[@class=\"metadata\"]/div[@class=\"attribute_snippet\"]').text\n",
    "            except:\n",
    "                type = None\n",
    "            types.append(type)\n",
    "            categories.append(keyword)\n",
    "\n",
    "            # print(f'{title}; {company}; {location}; {salary}; {type}')\n",
    "            \n",
    "        # Navigate to the next page\n",
    "        goto_nextpage(keyword, page)\n",
    "\n",
    "        \n",
    "    # Mark a keyword crawling process is done.\n",
    "    print('crawling completed.')\n",
    "\n",
    "    driver.get('https://www.indeed.com/stc?_ga=2.63393007.1966857820.1656152873-1453494806.1656152873')\n",
    "\n",
    "    # Start writing csv file process.\n",
    "    print('Starting wrtie to csv file....')\n",
    "    df_da=pd.DataFrame()\n",
    "    df_da['Title']=titles\n",
    "    df_da['Company']=companies\n",
    "    df_da['Location']=locations\n",
    "    df_da['Salary']=salaries\n",
    "    df_da['Type']=types\n",
    "    df_da['Category']=categories\n",
    "\n",
    "    df_da.to_csv(f'output\\\\part00{count}.csv', index=False)\n",
    "\n",
    "    # Increase count for next file\n",
    "    count += 1\n",
    "\n",
    "    # Fnish writing csv file process.\n",
    "    print(f'Finish write {keyword} data to csv file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c189afcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "endtime = time.time()\n",
    "process_time = round((endtime-starttime)/60,2)\n",
    "print(\"Processing Time: {} min\".format(process_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dec7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_files = glob.glob(\"output/*.csv\")\n",
    "# # print(all_files)\n",
    "# li = []\n",
    "\n",
    "# for filename in all_files:\n",
    "#     df = pd.read_csv(filename, index_col=None, header=0, delimiter=';')\n",
    "#     li.append(df)\n",
    "\n",
    "# df = pd.concat(li, axis=0, ignore_index=True)\n",
    "# print(frame)\n",
    "# df = df[['Title', 'Company', 'Location', 'Salary', 'Type', 'Category']].drop_duplicates()\n",
    "# print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
